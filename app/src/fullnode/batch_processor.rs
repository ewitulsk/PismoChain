//! Batch block processor for improved throughput
//! 
//! Processes multiple blocks in parallel to improve fullnode sync performance.

use std::{
    collections::VecDeque,
    sync::{Arc, Mutex},
    time::{Duration, Instant},
};
use tokio::sync::Semaphore;
use tracing::{info, warn, debug, error};
use hotstuff_rs::types::data_types::BlockHeight;

use crate::{
    networking::{FinalizedBlockMessage, BlockResponse},
    fullnode::{FullnodeApp, MetricsCollector},
    database::rocks_db::RocksDBStore,
};

/// Configuration for batch processing
#[derive(Debug, Clone)]
pub struct BatchConfig {
    /// Maximum number of blocks to process in parallel
    pub max_parallel_blocks: usize,
    /// Maximum size of the processing queue
    pub max_queue_size: usize,
    /// Timeout for block processing
    pub processing_timeout: Duration,
    /// Batch size for sequential processing
    pub batch_size: usize,
}

impl Default for BatchConfig {
    fn default() -> Self {
        Self {
            max_parallel_blocks: 10,
            max_queue_size: 1000,
            processing_timeout: Duration::from_secs(30),
            batch_size: 50,
        }
    }
}

/// Batch processor for handling multiple blocks efficiently
pub struct BatchBlockProcessor {
    config: BatchConfig,
    fullnode_app: Arc<Mutex<FullnodeApp>>,
    kv_store: Arc<RocksDBStore>,
    metrics_collector: Option<Arc<MetricsCollector>>,
    /// Processing queue for incoming blocks
    processing_queue: Arc<Mutex<VecDeque<FinalizedBlockMessage>>>,
    /// Semaphore to limit concurrent processing
    processing_semaphore: Arc<Semaphore>,
    /// Track processing statistics
    total_processed: Arc<Mutex<u64>>,
    total_processing_time: Arc<Mutex<Duration>>,
}

impl BatchBlockProcessor {
    /// Create a new batch processor
    pub fn new(
        config: BatchConfig,
        fullnode_app: Arc<Mutex<FullnodeApp>>,
        kv_store: Arc<RocksDBStore>,
        metrics_collector: Option<Arc<MetricsCollector>>,
    ) -> Self {
        let processing_semaphore = Arc::new(Semaphore::new(config.max_parallel_blocks));
        
        Self {
            config,
            fullnode_app,
            kv_store,
            metrics_collector,
            processing_queue: Arc::new(Mutex::new(VecDeque::new())),
            processing_semaphore,
            total_processed: Arc::new(Mutex::new(0)),
            total_processing_time: Arc::new(Mutex::new(Duration::ZERO)),
        }
    }

    /// Add a block to the processing queue
    pub fn queue_block(&self, block: FinalizedBlockMessage) -> anyhow::Result<()> {
        if let Ok(mut queue) = self.processing_queue.lock() {
            if queue.len() >= self.config.max_queue_size {
                warn!("‚ö†Ô∏è Block processing queue full, dropping oldest block");
                queue.pop_front();
            }
            
            queue.push_back(block);
            debug!("üì• Queued block for batch processing (queue size: {})", queue.len());
            
            // Update metrics
            if let Some(ref collector) = self.metrics_collector {
                collector.update_queue_depth(queue.len());
            }
        }
        
        Ok(())
    }

    /// Process a batch of blocks from the queue
    pub async fn process_batch(&self) -> anyhow::Result<usize> {
        let blocks_to_process = {
            let mut queue = self.processing_queue.lock()
                .map_err(|e| anyhow::anyhow!("Failed to lock processing queue: {}", e))?;
            
            let batch_size = self.config.batch_size.min(queue.len());
            let mut batch = Vec::with_capacity(batch_size);
            
            for _ in 0..batch_size {
                if let Some(block) = queue.pop_front() {
                    batch.push(block);
                }
            }
            
            batch
        };

        if blocks_to_process.is_empty() {
            return Ok(0);
        }

        info!("‚ö° Processing batch of {} blocks", blocks_to_process.len());
        let batch_start = Instant::now();

        // Process blocks in parallel with semaphore limiting
        let mut tasks = Vec::new();
        for block in blocks_to_process {
            let semaphore = self.processing_semaphore.clone();
            let fullnode_app = self.fullnode_app.clone();
            let metrics = self.metrics_collector.clone();
            let total_processed = self.total_processed.clone();
            let total_time = self.total_processing_time.clone();
            
            let task = tokio::spawn(async move {
                let _permit = semaphore.acquire().await.unwrap();
                let start_time = Instant::now();
                
                // Process the block (simplified validation)
                let result = Self::process_single_block(block, fullnode_app).await;
                
                let processing_time = start_time.elapsed();
                
                // Update metrics
                if let Some(collector) = metrics {
                    collector.record_block_processing(processing_time);
                }
                
                // Update totals
                if let Ok(mut count) = total_processed.lock() {
                    *count += 1;
                }
                if let Ok(mut time) = total_time.lock() {
                    *time += processing_time;
                }
                
                result
            });
            
            tasks.push(task);
        }

        // Wait for all tasks to complete
        let mut successful = 0;
        let mut failed = 0;
        
        for task in tasks {
            match task.await {
                Ok(Ok(_)) => successful += 1,
                Ok(Err(e)) => {
                    error!("‚ùå Block processing failed: {}", e);
                    failed += 1;
                }
                Err(e) => {
                    error!("‚ùå Block processing task panicked: {}", e);
                    failed += 1;
                }
            }
        }

        let batch_time = batch_start.elapsed();
        info!("‚úÖ Batch complete: {} successful, {} failed in {:?}", 
            successful, failed, batch_time);

        Ok(successful)
    }

    /// Process a single block (internal helper)
    async fn process_single_block(
        block: FinalizedBlockMessage,
        _fullnode_app: Arc<Mutex<FullnodeApp>>,
    ) -> anyhow::Result<()> {
        // This is a simplified implementation
        // In practice, you'd want to:
        // 1. Deserialize and validate the block
        // 2. Apply state changes  
        // 3. Update storage
        
        // For now, just simulate processing without holding locks across await
        debug!("‚úÖ Processing block {}", block.block_height);
        
        // Simulate processing time
        tokio::time::sleep(Duration::from_millis(1)).await;
        
        debug!("‚úÖ Processed block {}", block.block_height);
        Ok(())
    }

    /// Get processing statistics
    pub fn get_stats(&self) -> BatchProcessingStats {
        let total_processed = self.total_processed.lock()
            .map(|count| *count)
            .unwrap_or(0);
        
        let total_time = self.total_processing_time.lock()
            .map(|time| *time)
            .unwrap_or(Duration::ZERO);
        
        let queue_size = self.processing_queue.lock()
            .map(|queue| queue.len())
            .unwrap_or(0);

        let avg_processing_time = if total_processed > 0 {
            total_time / total_processed as u32
        } else {
            Duration::ZERO
        };

        let throughput = if total_time.as_secs_f32() > 0.0 {
            total_processed as f32 / total_time.as_secs_f32()
        } else {
            0.0
        };

        BatchProcessingStats {
            total_processed,
            queue_size,
            avg_processing_time,
            throughput,
            max_parallel: self.config.max_parallel_blocks,
        }
    }

    /// Start the background batch processing loop
    pub async fn start_processing_loop(self: Arc<Self>) {
        info!("üöÄ Starting batch block processing loop");
        
        let mut processing_interval = tokio::time::interval(Duration::from_millis(100));
        
        loop {
            processing_interval.tick().await;
            
            // Process a batch if there are blocks waiting
            match self.process_batch().await {
                Ok(processed_count) => {
                    if processed_count > 0 {
                        debug!("Processed {} blocks in batch", processed_count);
                    }
                }
                Err(e) => {
                    error!("‚ùå Batch processing error: {}", e);
                }
            }
        }
    }
}

/// Statistics for batch processing performance
#[derive(Debug, Clone)]
pub struct BatchProcessingStats {
    /// Total number of blocks processed
    pub total_processed: u64,
    /// Current queue size
    pub queue_size: usize,
    /// Average processing time per block
    pub avg_processing_time: Duration,
    /// Blocks processed per second
    pub throughput: f32,
    /// Maximum parallel processing slots
    pub max_parallel: usize,
}

#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;

    #[tokio::test]
    async fn test_batch_config_default() {
        let config = BatchConfig::default();
        assert_eq!(config.max_parallel_blocks, 10);
        assert_eq!(config.max_queue_size, 1000);
        assert_eq!(config.batch_size, 50);
    }

    #[tokio::test]
    async fn test_queue_management() {
        let temp_dir = TempDir::new().unwrap();
        let kv_store = RocksDBStore::new(temp_dir.path().join("test_db").to_str().unwrap()).unwrap();
        let fullnode_app = Arc::new(Mutex::new(
            FullnodeApp::new(
                crate::config::Config {
                    chain_id: 1,
                    network: "test".to_string(),
                    sui: crate::config::Sui {
                        pismo_locker_address: "test".to_string(),
                    },
                    node_mode: crate::types::NodeMode::Fullnode,
                },
                crate::standards::book_executor::BookExecutor::new(),
                1
            )
        ));

        let processor = BatchBlockProcessor::new(
            BatchConfig::default(),
            fullnode_app,
            Arc::new(kv_store),
            None,
        );

        // Test queue management
        let stats_before = processor.get_stats();
        assert_eq!(stats_before.queue_size, 0);

        // Add a test block
        let test_block = FinalizedBlockMessage {
            block_height: 1,
            block_hash: [0u8; 32],
            block_data: vec![],
            payload_data: vec![],
        };

        processor.queue_block(test_block).unwrap();
        
        let stats_after = processor.get_stats();
        assert_eq!(stats_after.queue_size, 1);
    }
}
